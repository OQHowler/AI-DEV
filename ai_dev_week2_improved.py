# -*- coding: utf-8 -*-
"""AI_DEV_week2_IMPROVED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MvzIBhlA3zWrwO6UZ1h3Z2QPe1_Da_VR

This is my THIRD attempt at making this model.In the first attempt i did not use BERT

#Attempt 1
normal :-
 MAE: 814.313290713805                 
 ---------------RMSE: 3284.8791334935618

#Attempt 2
used log1p(likes):- MAE: 541.7342766493772              
--------------------RMSE: 2953.6572770854145

#Attempt 3
using bert and log of lieks:- MAE: 602.95                  
--------------------------------------------RMSE: 3177.70
"""

!pip install -q sentence-transformers


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load the CSV file (replace with your actual file path)
df = pd.read_csv("behaviour_simulation_train.xlsx - Sheet1 (1).csv")  # replace with your actual path

# Basic data inspection
print(df.shape)         # Shows number of rows and columns
print(df.head())        # Displays the first 5 rows
print(df.info())        # Provides summary info of the DataFrame
print(df.isnull().sum())  # Counts missing values in each column

df.dropna(subset=['content', 'username', 'inferred company', 'likes'], inplace=True)
df['media'].fillna('no_media', inplace=True)
df['has_media'] = df['media'].apply(lambda x: x != 'no_media')
df['content'] = df['content'].astype(str).str.strip().str.lower()
df['datetime'] = pd.to_datetime(df['date'], errors='coerce')

!pip install emoji

import emoji
import re

#  I counted number of emojis in the tweet
def count_emojis(text):
    return sum(1 for char in str(text) if char in emoji.EMOJI_DATA)

# then checked the hastags
def has_hashtag(text):
    return 1 if re.search(r"#\w+", str(text)) else 0


df['emoji_count'] = df['content'].apply(count_emojis)
df['has_hashtag'] = df['content'].apply(has_hashtag)

#  Adding Sentiment
!pip install -q textblob
from textblob import TextBlob
df['sentiment'] = df['content'].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)

df['word_count'] = df['content'].apply(lambda x: len(str(x).split()))
df['char_count'] = df['content'].apply(lambda x: len(str(x)))
df['has_media'] = df['media'].apply(lambda x: 1 if x else 0)

df['datetime'] = pd.to_datetime(df['datetime'])
df['hour'] = df['datetime'].dt.hour
df['day_of_week'] = df['datetime'].dt.dayofweek


df['company_encoded'] = LabelEncoder().fit_transform(df['inferred company'])
df['username_encoded'] = LabelEncoder().fit_transform(df['username'])

print("Generating BERT embeddings...")
model = SentenceTransformer('all-MiniLM-L6-v2')
bert_embeddings = model.encode(df['content'].astype(str).tolist(), show_progress_bar=True)


bert_df = pd.DataFrame(bert_embeddings, columns=[f'bert_{i}' for i in range(bert_embeddings.shape[1])])
df = pd.concat([df.reset_index(drop=True), bert_df], axis=1)

basic_features = ['sentiment', 'has_media', 'hour', 'day_of_week',
                  'company_encoded', 'username_encoded', 'emoji_count', 'has_hashtag']

bert_features = [f'bert_{i}' for i in range(bert_embeddings.shape[1])]
features = basic_features + bert_features

X = df[features]
y = np.log1p(df['likes'])  # log1p

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training
model = RandomForestRegressor()
model.fit(X_train, y_train)

# Prediction
y_pred_log = model.predict(X_test)
y_pred = np.expm1(y_pred_log)
y_true = np.expm1(y_test)

mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))

print(f"\nðŸ“Š Model with BERT Embeddings:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")



"""#next everything is chatgpt
i did not get enough time to read thru the resourses so i just asked chatgpt
"""

import joblib
joblib.dump(model, 'like_predictor.pkl')

!pip install flask-ngrok

from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
import joblib
import numpy as np

# Load the model
model = joblib.load('like_predictor.pkl')

# Create Flask app
app = Flask(__name__)
run_with_ngrok(app)  # starts ngrok when app is run

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()

    # Adjust features here based on your model
    features = np.array([
        data['word_count'],
        data['char_count'],
        data['sentiment'],
        data['has_media'],
        data['hour'],
        data['day_of_week'],
        data['company_encoded'],
        data['username_encoded'],
        data['emoji_count'],
        data['has_hashtag']
    ]).reshape(1, -1)

    prediction = model.predict(features)[0]
    return jsonify({'predicted_likes': int(prediction)})

# Start Flask app
app.run()